"""
VLA è¯„ä¼°é©±åŠ¨ï¼šåªè´Ÿè´£åˆ›å»ºç¯å¢ƒã€å¾ªç¯ stepã€æ”¶é›†/æ‰“å°æŒ‡æ ‡ã€‚
åŠ¨ä½œç”±å¤–éƒ¨æ³¨å…¥çš„ policy ç”Ÿæˆï¼Œè¯„ä¼°å™¨ä¸åŒ…å«ä»»ä½•ç­–ç•¥é€»è¾‘ã€‚
ç­–ç•¥æ”¶åˆ°çš„ obs ä¸º ObservationBuilder äº§å‡ºçš„ ObservationDictï¼ˆmeta / robot_state / vision / point_cloudï¼‰ï¼Œ
è€Œé env åŸç”Ÿçš„ group è§‚æµ‹ã€‚
"""

from .VLAIsaacEnv import VLAIsaacEnv
import torch
import numpy as np
import time
import imageio
from pathlib import Path

from isaaclab.utils.math import subtract_frame_transforms, combine_frame_transforms
from isaaclab_logistics_vla.evaluation.observation.builder import EpisodeContext, ObservationBuilder
from isaaclab_logistics_vla.evaluation.observation.schema import ObservationRequire
from isaaclab_logistics_vla.evaluation.result.saver import ResultSaver, EpisodeReport
from isaaclab_logistics_vla.evaluation.robot_registry import get_robot_eval_config
# Curobo é€†è¿åŠ¨å­¦æ±‚è§£å™¨ï¼ˆé…ç½®ä»æœ¬åŒ… configs/robot_configs/ åŠ è½½ï¼Œä¸ä¾èµ– Curobo å®‰è£…è·¯å¾„ï¼‰
from curobo.wrap.reacher.ik_solver import IKSolver, IKSolverConfig
from curobo.types.math import Pose
from curobo.types.robot import JointState, RobotConfig
from curobo.types.base import TensorDeviceType
from curobo.util_file import load_yaml

import isaaclab_logistics_vla


def _get_action_from_policy(policy, obs):
    """ä»ç­–ç•¥å¾—åˆ°åŠ¨ä½œï¼šæ”¯æŒ predict(obs) è¿”å› tensorï¼Œæˆ– __call__(obs) è¿”å› ActionDictã€‚"""
    if hasattr(policy, "predict"):
        out = policy.predict(obs, **{})
        return out if isinstance(out, torch.Tensor) else out.get("action", out)
    out = policy(obs)
    if isinstance(out, dict) and "action" in out:
        return out["action"]
    return out


def _make_policy_from_name(
    policy_name: str,
    env,
    trajectory_path: str = None,
    robot_eval_cfg=None,
):
    """æ ¹æ®åç§°åˆ›å»ºç­–ç•¥å®ä¾‹ï¼Œç”¨äºè„šæœ¬ä¼  --policy å­—ç¬¦ä¸²æ—¶ã€‚
    robot_eval_cfg: å¯é€‰ï¼ŒRobotEvalConfigï¼›ç”¨äº OpenVLA ç­‰ç­–ç•¥æ—¶ä¼ å…¥è¯¥æœºå™¨äººçš„ unnorm_keyã€‚
    """
    action_dim = env.unwrapped.action_manager.total_action_dim
    device = env.device
    if policy_name == "random":
        from isaaclab_logistics_vla.evaluation.models.policy.random_policy import RandomPolicy
        return RandomPolicy(action_dim=action_dim, device=device)
    if policy_name in ("openpi", "pi0", "openpi_remote"):
        from isaaclab_logistics_vla.evaluation.models.policy.openpi_remote_policy import OpenPIRemotePolicy
        return OpenPIRemotePolicy(action_dim=action_dim, device=device)
    if policy_name == "openvla_stub":
        # ä»…æœ¬åœ°è°ƒè¯•ç”¨ï¼Œä¸åšè¿œç¨‹è°ƒç”¨
        from isaaclab_logistics_vla.evaluation.models.policy.openvla_stub_policy import OpenVLAStubPolicy
        return OpenVLAStubPolicy(action_dim=action_dim, device=device)
    if policy_name == "openvla":
        # çœŸå®è¿œç¨‹ OpenVLAï¼šèµ° deploy.py çš„ HTTP /act æ¥å£ï¼›unnorm_key æŒ‰ robot_id ä»æ³¨å†Œè¡¨å–ï¼Œä¸åŠ¨ä½œç»´åº¦å¯¹åº”
        from isaaclab_logistics_vla.evaluation.models.policy.openvla_remote_policy import OpenVLARemotePolicy
        unnorm_key = getattr(robot_eval_cfg, "unnorm_key", None) if robot_eval_cfg else None
        if unnorm_key is None:
            unnorm_key = "bridge_orig"
        return OpenVLARemotePolicy(action_dim=action_dim, device=device, unnorm_key=unnorm_key)
    if policy_name in ("rrt", "trajectory"):
        from isaaclab_logistics_vla.evaluation.models.policy.trajectory_playback_policy import TrajectoryPlaybackPolicy
        path = trajectory_path or "/home/wst/code/ompl/RRT_path.txt"
        return TrajectoryPlaybackPolicy(txt_path=path, device=device, action_dim=action_dim, lift_duration=250)
    raise ValueError(f"Unknown policy name: {policy_name!r}. Use 'random', 'rrt', or 'trajectory'.")


class VLA_Evaluator:
    """çº¯é©±åŠ¨ï¼šæŒæœ‰ä¸€ä¸ª env å’Œä¸€ä¸ª policyï¼Œrun_evaluation é‡Œåªåš reset â†’ å¾ªç¯(å– obs â†’ policy å¾— action â†’ step)ã€‚"""

    def __init__(
        self,
        env_cfg,
        policy,
        trajectory_path: str = None,
        record_video: bool = True,
        video_output_dir: str = "./videos",
        robot_id: str = "realman_dual_left_arm",
        from_json: int = 0,
    ):
        """
        Args:
            env_cfg: ç¯å¢ƒé…ç½®ï¼ˆå¦‚ OrderEnvCfg()ï¼‰
            policy: ç­–ç•¥å®ä¾‹ï¼Œæˆ–ç­–ç•¥åç§°ã€‚
            trajectory_path: è½¨è¿¹è·¯å¾„
            record_video: æ˜¯å¦å½•åˆ¶è§†é¢‘
            video_output_dir: è§†é¢‘è¾“å‡ºç›®å½•
            robot_id: è¯„ä¼°ä¾§æœºå™¨äºº IDï¼Œç”¨äºä» robot_registry å– arm_dof / å¹³å°å…³èŠ‚ / Curobo é…ç½®ç­‰ã€‚
                è§ evaluation/robot_registry.pyï¼Œæ–°æœºå™¨äººéœ€åœ¨ REGISTRY ä¸­æ³¨å†Œã€‚
            from_json: 0=è®°å½• JSONï¼Œ1=å›æ”¾ JSONï¼Œ2=çº¯éšæœºï¼ˆä¸ scripts/evaluate_vla.py --from_json å¯¹åº”ï¼‰ã€‚
        """
        self.env = VLAIsaacEnv(cfg=env_cfg)
        # è¯´æ˜ï¼šåœºæ™¯ä¸­çš„æœºå™¨äººç”± env_cfg.scene åŠ è½½ï¼ˆOrderSceneCfg â†’ register.load_robot('realman_franka_ee')ï¼‰ï¼Œ
        # ä¸ robot_id/robot_registry æ— å…³ã€‚robot_registry ä»…ç”¨äºè¯„ä¼°ä¾§ IKã€arm_dofã€å¹³å°å…³èŠ‚åç­‰ã€‚
        _pkg_dir = Path(isaaclab_logistics_vla.__file__).resolve().parent
        _scene_robot_usd = _pkg_dir / "assets" / "robots" / "realman" / "realman_franka_ee.usd"
        print(f"[INFO] åœºæ™¯æœºå™¨äººç”± env_cfg åŠ è½½ï¼ŒUSD è·¯å¾„: {_scene_robot_usd} (å­˜åœ¨: {_scene_robot_usd.exists()})")
        self._robot_eval_cfg = get_robot_eval_config(robot_id)
        if isinstance(policy, str):
            self.policy = _make_policy_from_name(
                policy, self.env, trajectory_path, robot_eval_cfg=self._robot_eval_cfg
            )
        else:
            self.policy = policy
        self.isprint = False
        self.from_json = from_json  # 0: è®°å½• JSON, 1: å›æ”¾ JSON, 2: çº¯éšæœº

        # Observation Builder
        self._obs_builder = ObservationBuilder(self.env)
        self._obs_require = ObservationRequire(
            require_rgb=True,
            require_depth=True,
            require_seg=True,
            require_pcd=False,
            pcd_frame="camera",
        )
        sensors = getattr(self.env.unwrapped.scene, "sensors", {})
        self._camera_names = sorted([n for n in sensors.keys() if "camera" in n.lower()]) or None
        
        # è§†é¢‘å½•åˆ¶è®¾ç½®
        self.record_video = record_video
        self.video_writers = {}
        self.video_output_dir = Path(video_output_dir)
        self.video_output_dir.mkdir(parents=True, exist_ok=True)
        self.video_initialized = False
        
        # ResultSaver åˆå§‹åŒ–
        self.result_saver = ResultSaver(output_dir="./results")

    
        self.ik_solver = None
        self._retract_config_list = None
        self.arm_dof = self._robot_eval_cfg.arm_dof

        if self._robot_eval_cfg.curobo_yml_name and self._robot_eval_cfg.curobo_asset_folder and self._robot_eval_cfg.curobo_urdf_name:
            try:
                print(f"ğŸ”„ åˆå§‹åŒ– Curobo IK Solver (robot_id={robot_id})...")
                tensor_args = TensorDeviceType(device=self.env.device)
                _pkg_dir = Path(isaaclab_logistics_vla.__file__).resolve().parent
                _robot_configs_dir = _pkg_dir / "configs" / "robot_configs"
                _robot_yml = _robot_configs_dir / self._robot_eval_cfg.curobo_yml_name
                config_file = load_yaml(str(_robot_yml))
                _assets_dir = _pkg_dir / "assets" / "robots" / self._robot_eval_cfg.curobo_asset_folder
                config_file["robot_cfg"]["kinematics"]["urdf_path"] = str(_assets_dir / self._robot_eval_cfg.curobo_urdf_name)
                config_file["robot_cfg"]["kinematics"]["asset_root_path"] = str(_assets_dir)
                config_file["robot_cfg"]["kinematics"]["collision_spheres"] = str(
                    _robot_configs_dir / "spheres" / self._robot_eval_cfg.curobo_yml_name
                )
                robot_cfg = RobotConfig.from_dict(config_file["robot_cfg"], tensor_args)
                ik_config = IKSolverConfig.load_from_robot_config(
                    robot_cfg,
                    None,
                    rotation_threshold=0.05,
                    position_threshold=0.01,
                    num_seeds=32,
                    self_collision_check=True,
                    self_collision_opt=True,
                    tensor_args=tensor_args,
                    use_cuda_graph=True,
                )
                self.ik_solver = IKSolver(ik_config)
                self._retract_config_list = config_file["robot_cfg"]["kinematics"].get("cspace", {}).get("retract_config")
                print("âœ… Curobo IK Solver åˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                print(f"âŒ Curobo åˆå§‹åŒ–å¤±è´¥: {e}")
                self.ik_solver = None
                self._retract_config_list = None
        else:
            print(f"[INFO] robot_id={robot_id} æœªé…ç½® Curoboï¼ˆcurobo_yml_name/asset/urdf ä¸ºç©ºï¼‰ï¼ŒEE æ¨¡å¼ä¸å¯ç”¨ã€‚")

    def _init_video_writers(self, obs_dict):
        """åˆå§‹åŒ–è§†é¢‘å†™å…¥å™¨"""
        if not self.record_video:
            return
        
        try:
            # ä»obs_dictè·å–visionæ•°æ®
            vision = obs_dict.get("vision", {})
            cameras = vision.get("cameras", [])
            rgb = vision.get("rgb", None)
            
            if rgb is not None and len(cameras) > 0:
                # è·å–æ—¶é—´æˆ³
                timestamp = int(time.time())
                fps = 20  # å½•åˆ¶å¸§ç‡
                
                # ä¸ºæ¯ä¸ªç›¸æœºåˆ›å»ºè§†é¢‘å†™å…¥å™¨
                for cam_idx, cam_name in enumerate(cameras):
                    # è·å–å›¾åƒå½¢çŠ¶
                    height, width = rgb.shape[2], rgb.shape[3]  # (ç›¸æœºæ•°, ç¯å¢ƒæ•°, é«˜åº¦, å®½åº¦, é€šé“)
                    
                    # åˆ›å»ºè§†é¢‘æ–‡ä»¶å
                    video_filename = f"{cam_name}_{timestamp}.mp4"
                    video_path = self.video_output_dir / video_filename
                    
                    # ä½¿ç”¨imageioåˆ›å»ºè§†é¢‘å†™å…¥å™¨
                    video_writer = imageio.get_writer(
                        str(video_path),
                        fps=fps,
                        codec='libx264',
                        quality=9
                    )
                    
                    self.video_writers[cam_name] = video_writer
                    print(f"ğŸ¥ {cam_name} è§†é¢‘å½•åˆ¶å·²åˆå§‹åŒ–: {video_path}")
                    print(f"ğŸ“¹ å½•åˆ¶å‚æ•°: {width}x{height}, {fps}fps")
                
                self.video_initialized = True
        except Exception as e:
            print(f"âš ï¸ è§†é¢‘å†™å…¥å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

    def _record_frame_from_obs(self, obs_dict):
        """ä»obs_dictå½•åˆ¶è§†é¢‘å¸§"""
        if not self.record_video:
            return
        
        # åˆå§‹åŒ–è§†é¢‘å†™å…¥å™¨
        if not self.video_initialized:
            self._init_video_writers(obs_dict)
            if not self.video_initialized:
                return
        
        try:
            # ä»obs_dictè·å–visionæ•°æ®
            vision = obs_dict.get("vision", {})
            cameras = vision.get("cameras", [])
            rgb = vision.get("rgb", None)
            
            if rgb is not None and len(cameras) > 0:
                # ä¸ºæ¯ä¸ªç›¸æœºå½•åˆ¶å¸§
                for cam_idx, cam_name in enumerate(cameras):
                    if cam_name in self.video_writers:
                        # è·å–å½“å‰ç›¸æœºçš„å›¾åƒ
                        frame = rgb[cam_idx, 0].cpu().numpy()  # (H, W, 3)
                        
                        # ç¡®ä¿å›¾åƒæ•°æ®ç±»å‹æ­£ç¡®
                        if frame.dtype == np.float32:
                            frame = (frame * 255).astype(np.uint8)
                        elif frame.dtype != np.uint8:
                            frame = frame.astype(np.uint8)
                        
                        # å†™å…¥è§†é¢‘
                        writer = self.video_writers[cam_name]
                        writer.append_data(frame)
        except Exception as e:
            print(f"âš ï¸ è§†é¢‘å½•åˆ¶é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()

    def close_video_recording(self):
        """å…³é—­è§†é¢‘å½•åˆ¶"""
        for cam_name, writer in self.video_writers.items():
            if writer is not None:
                writer.close()
                print(f"ğŸ¬ {cam_name} è§†é¢‘å½•åˆ¶å·²å®Œæˆ")
        self.video_writers.clear()
    
    def _save_evaluation_result(self, start_time, episode_length, info, rew, terminated, truncated, ctx):
        """
        ä¿å­˜è¯„ä¼°ç»“æœ
        
        Args:
            start_time: è¯„ä¼°å¼€å§‹æ—¶é—´
            episode_length: episode é•¿åº¦
            info: ç¯å¢ƒè¿”å›çš„ä¿¡æ¯
            rew: å¥–åŠ±
            terminated: æ˜¯å¦æ­£å¸¸ç»ˆæ­¢
            truncated: æ˜¯å¦è¢«æˆªæ–­
            ctx:  episode ä¸Šä¸‹æ–‡
        """
        try:
            # è®¡ç®—è¯„ä¼°æ—¶é—´
            eval_time = time.time() - start_time
            
            # æ„å»º metrics_readï¼ˆè¿™é‡Œä½¿ç”¨ info ä½œä¸ºç¤ºä¾‹ï¼Œå®é™…åº”ä»ç¯å¢ƒè¯»å–ï¼‰
            metrics_read = info.get("metrics", {})
            if not metrics_read:
                # å¦‚æœæ²¡æœ‰ metricsï¼Œä½¿ç”¨ç®€å•çš„å¥–åŠ±ä½œä¸ºç¤ºä¾‹
                metrics_read = {"total_reward": float(rew.sum().item())}
            
            # æ„å»º timing ä¿¡æ¯
            timing = {
                "episode_time": eval_time,
                "steps_per_second": episode_length / eval_time if eval_time > 0 else 0
            }
            
            # æ„å»ºå¹¶ä¿å­˜ episode æŠ¥å‘Š
            episode_report = EpisodeReport(
                episode_id=ctx.episode_id,
                seed=None,  # å¯ä»¥ä» env ä¸­è·å–
                success=bool(terminated and not truncated),  # å‡è®¾ terminated è¡¨ç¤ºæˆåŠŸ
                metrics_read=metrics_read,
                timing=timing,
                task_name=ctx.task_name,
                episode_length=episode_length
            )
            
            # ä¿å­˜ episode ç»“æœ
            self.result_saver.write_episode(episode_report)
            
            # ç”Ÿæˆå¹¶ä¿å­˜ä»»åŠ¡æŠ¥å‘Š
            self.result_saver.write_task(task_name=ctx.task_name)
            
            print(f"\nğŸ“Š è¯„ä¼°å®Œæˆ:")
            print(f"  - Episode é•¿åº¦: {episode_length} æ­¥")
            print(f"  - è¯„ä¼°æ—¶é—´: {eval_time:.2f} ç§’")
            print(f"  - æˆåŠŸç‡: {'âœ“' if episode_report.success else 'âœ—'}")
            print(f"  - å¥–åŠ±: {metrics_read.get('total_reward', 0):.2f}")
            print(f"  - ç»“æœå·²ä¿å­˜åˆ°: {self.result_saver.output_dir}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜è¯„ä¼°ç»“æœå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

    def _convert_actions_by_control_mode(self, actions, obs_dict):
        """
        æ ¹æ®ç­–ç•¥çš„æ§åˆ¶æ¨¡å¼è½¬æ¢åŠ¨ä½œæ ¼å¼
        """
        control_mode = getattr(self.policy, "control_mode", "joint")
        
        if control_mode == "ee":
            # EE æ¨¡å¼å¿…é¡»è¦æœ‰ IKï¼Œå¦åˆ™æŠ¥é”™ï¼ˆé™é»˜è¿”å›é”™è¯¯åŠ¨ä½œæ— æ„ä¹‰ï¼‰
            if not self.ik_solver:
                raise RuntimeError(
                    "EE æ¨¡å¼ä¸‹ Curobo IK æœªåˆå§‹åŒ–æˆåŠŸï¼Œæ— æ³•å°†æœ«ç«¯åŠ¨ä½œè½¬ä¸ºå…³èŠ‚åŠ¨ä½œã€‚"
                    " è¯·æ£€æŸ¥ configs/robot_configs/ ä¸ Curobo ä¾èµ–ã€‚"
                )

            # è·å– robot_stateï¼ˆç¼º qpos æ— æ³•åš IKï¼Œç›´æ¥æŠ¥é”™ï¼‰
            robot_state = obs_dict.get("robot_state", {})
            if not robot_state or "qpos" not in robot_state:
                raise RuntimeError(
                    "EE æ¨¡å¼ä¸‹ obs_dict ç¼ºå°‘ robot_state.qposï¼Œæ— æ³•åš IKã€‚"
                    " è¯·ç¡®ä¿ ObservationBuilder æä¾› robot_stateã€‚"
                )

            current_qpos = robot_state["qpos"]
            
            # è·å–/è®¡ç®— target_ee_pos
            robot_data = self.env.unwrapped.scene.articulations["robot"].data
            
            # è·å–å½“å‰æœ«ç«¯ä½ç½®ä¸å§¿æ€ï¼ˆç¼ºä¸€ä¸å¯ï¼Œå¦åˆ™ IK æ— æ„ä¹‰ï¼‰
            ee_pos = None
            ee_quat = None
            if hasattr(robot_data, "target_pos_w") and hasattr(robot_data, "body_state_w"):
                ee_pos = robot_data.target_pos_w[:, 0, :]
                ee_quat = robot_data.body_state_w[:, -1, 3:7]
            elif hasattr(robot_data, "body_state_w"):
                ee_pos = robot_data.body_state_w[:, -1, 0:3]
                ee_quat = robot_data.body_state_w[:, -1, 3:7]
            if ee_pos is None or ee_quat is None:
                raise RuntimeError(
                    "EE æ¨¡å¼ä¸‹æ— æ³•è·å–æœ«ç«¯ä½å§¿ï¼ˆee_pos / ee_quatï¼‰ã€‚"
                    " robot_data éœ€æä¾› target_pos_w + body_state_w æˆ– body_state_wã€‚"
                )

            # ç­–ç•¥ç»™å‡ºçš„æœ«ç«¯ä½ç§»å¢é‡ï¼ˆå‰ 3 ç»´ï¼Œå•ä½éœ€ä¸ ee_pos ä¸€è‡´ï¼Œä¸€èˆ¬ä¸ºç±³ï¼‰
            ee_delta = actions[:, :3]
            target_ee_pos_w = ee_pos + ee_delta

            # Curobo çš„ URDF åŸºåº§åœ¨åŸç‚¹ï¼Œéœ€æŠŠç›®æ ‡ä½ç½®ä» Isaac ä¸–ç•Œç³»å˜æ¢åˆ°è‡‚åŸºåº§ç³»ï¼ˆå«å¹³å°é«˜åº¦ï¼‰
            root_pos_w = robot_data.root_pos_w[:, :3]
            root_quat_w = robot_data.root_quat_w
            # è‹¥æœ‰å¯ç§»åŠ¨å¹³å°ï¼ˆä»æ³¨å†Œè¡¨å– platform_joint_nameï¼‰ï¼Œè‡‚åŸº = root + [0,0,platform_height]ï¼ˆroot ç³»ä¸‹ï¼‰
            arm_base_pos_w = root_pos_w.clone()
            platform_joint_name = getattr(self._robot_eval_cfg, "platform_joint_name", None)
            if platform_joint_name and hasattr(robot_data, "joint_names") and platform_joint_name in robot_data.joint_names:
                platform_idx = list(robot_data.joint_names).index(platform_joint_name)
                platform_pos = robot_data.joint_pos[:, platform_idx]  # (num_envs,)
                offset_in_root = torch.zeros(
                    platform_pos.shape[0], 3,
                    dtype=root_pos_w.dtype, device=root_pos_w.device
                )
                offset_in_root[:, 2] = platform_pos
                arm_base_pos_w, _ = combine_frame_transforms(
                    root_pos_w, root_quat_w, offset_in_root
                )
            # ä½ç½®ä¸å§¿æ€éƒ½å˜æ¢åˆ°è‡‚åŸºç³»ï¼ˆCurobo æœŸæœ›ä½å§¿å‡åœ¨åŸºåº§ç³»ï¼‰
            target_ee_pos_b, ee_quat_b = subtract_frame_transforms(
                arm_base_pos_w, root_quat_w, target_ee_pos_w, ee_quat
            )
            target_ee_pos = target_ee_pos_b
            ee_quat_for_pose = ee_quat_b

            # Debugï¼šæ‰“å°ä¸–ç•Œç³»ä¸åŸºåº§ç³»ä¸‹çš„æœ«ç«¯ã€ç›®æ ‡ã€å½“å‰å…³èŠ‚è§’
            _ee_w = ee_pos[0].detach().cpu().numpy()
            _ee_b = target_ee_pos_b[0].detach().cpu().numpy()
            _delta = ee_delta[0].detach().cpu().numpy()
            _target_w = target_ee_pos_w[0].detach().cpu().numpy()
            _target_b = target_ee_pos_b[0].detach().cpu().numpy()
            _q = current_qpos[0, : self.arm_dof].detach().cpu().numpy()
            print("[IK] å½“å‰æœ«ç«¯ä½ç½® ä¸–ç•Œç³» (m):", _ee_w.tolist())
            print("[IK] ç›®æ ‡æœ«ç«¯ä½ç½® ä¸–ç•Œç³» (m):", _target_w.tolist())
            print("[IK] ç›®æ ‡æœ«ç«¯ä½ç½® åŸºåº§ç³» (m):", _target_b.tolist())
            print("[IK] ç­–ç•¥ä½ç§»å¢é‡ actions[:, :3]:", _delta.tolist())
            print("[IK] å½“å‰å·¦è‡‚å…³èŠ‚è§’ (rad):", _q.tolist())

            try:
                with torch.enable_grad():
                    target_pose = Pose(
                        target_ee_pos.detach().clone(),
                        ee_quat_for_pose.detach().clone()
                    )

                    arm_qpos = current_qpos[:, : self.arm_dof].detach().clone()
                    degenerate_threshold = 0.01
                    is_degenerate = (
                        arm_qpos.shape[1] >= 2
                        and (arm_qpos[:, 1:].abs() < degenerate_threshold).all().item()
                    )
                    # å‡†å¤‡å¤šç»„ seedï¼Œä¾æ¬¡å°è¯•ä»¥æé«˜æ”¶æ•›ç‡
                    seeds_to_try = []
                    if getattr(self, "_retract_config_list", None) is not None:
                        retract = torch.tensor(
                            self._retract_config_list,
                            dtype=arm_qpos.dtype,
                            device=arm_qpos.device,
                        ).unsqueeze(0).unsqueeze(1)  # (1, 1, 7)
                        if target_ee_pos.is_cuda:
                            retract = retract.to(target_ee_pos.device)
                        seeds_to_try.append(("retract_config", retract))
                    if arm_qpos.dim() == 2:
                        seed_current = arm_qpos.unsqueeze(1)
                    else:
                        seed_current = arm_qpos
                    if target_ee_pos.is_cuda:
                        seed_current = seed_current.to(target_ee_pos.device)
                    seeds_to_try.append(("current_qpos", seed_current))
                    # é›¶ä½ seedï¼ˆ7 è‡ªç”±åº¦ï¼‰
                    zero_seed = torch.zeros(
                        1, 1, self.arm_dof,
                        dtype=arm_qpos.dtype, device=arm_qpos.device
                    )
                    if target_ee_pos.is_cuda:
                        zero_seed = zero_seed.to(target_ee_pos.device)
                    seeds_to_try.append(("zero", zero_seed))

                    result = None
                    used_seed_name = None
                    print(f"[IK] ä¾æ¬¡å°è¯• {len(seeds_to_try)} ç»„ seed: {[s[0] for s in seeds_to_try]}")
                    for seed_name, seed_input in seeds_to_try:
                        print(f"[IK] å°è¯• seed={seed_name} ...", end=" ", flush=True)
                        result = self.ik_solver.solve_single(
                            target_pose,
                            seed_config=seed_input,
                            retract_config=seed_input
                        )
                        if result.success.item():
                            used_seed_name = seed_name
                            print("æ”¶æ•›")
                            break
                        print("æœªæ”¶æ•›")
                    if is_degenerate and used_seed_name:
                        print(f"[IK] å½“å‰å…³èŠ‚æ„å‹é€€åŒ–ï¼Œä½¿ç”¨ seed={used_seed_name} æ”¶æ•›")

                    if result is not None and result.success.item():
                        # å½“å‰ç¯å¢ƒ Curoboï¼šresult.solution ç›´æ¥ä¸ºå…³èŠ‚è§£ Tensorï¼ˆé JointState.positionï¼‰
                        sol_qpos = result.solution.detach()
                        if sol_qpos.dim() == 3:
                            sol_qpos = sol_qpos.squeeze(1)
                        new_actions = actions.clone()
                        if new_actions.shape[1] >= self.arm_dof:
                            new_actions[:, : self.arm_dof] = sol_qpos
                            return new_actions
                        raise RuntimeError(
                            f"IK æˆåŠŸä½† action ç»´åº¦ä¸è¶³: new_actions.shape[1]={new_actions.shape[1]}, arm_dof={self.arm_dof}"
                        )

                    # å¤šç»„ seed å‡æœªæ”¶æ•›ï¼šè·³è¿‡æœ¬æ­¥ï¼Œä¿æŒå½“å‰å…³èŠ‚è§’ï¼Œç»§ç»­ä¸‹ä¸€å¸§
                    print(f"[IK] å·²å°è¯• {len(seeds_to_try)} ç»„ seedï¼Œå‡æœªæ”¶æ•›ï¼›è·³è¿‡æœ¬æ­¥ï¼Œä¿æŒå½“å‰å…³èŠ‚è§’ç»§ç»­ä¸‹ä¸€å¸§")
                    new_actions = actions.clone()
                    new_actions[:, : self.arm_dof] = current_qpos[:, : self.arm_dof]
                    return new_actions

            except RuntimeError:
                raise
            except Exception as e:
                import traceback
                print(f"[IK] Curobo æ±‚è§£å¼‚å¸¸: {e}")
                traceback.print_exc()
                err_detail = (
                    f"å½“å‰æœ«ç«¯ (m): {_ee_w.tolist()}, ç›®æ ‡ (m): {_target_w.tolist()}, "
                    f"ä½ç§»å¢é‡: {_delta.tolist()}, å½“å‰å…³èŠ‚ (rad): {_q.tolist()}"
                )
                raise RuntimeError(
                    f"EE æ¨¡å¼ä¸‹ Curobo IK æ±‚è§£å¼‚å¸¸: {e}\n  {err_detail}"
                ) from e
        
        else:
            return actions

    def run_evaluation(self):
        step_i = 0
        episode_length = 0
        self.env.reset()
        if hasattr(self.policy, "reset"):
            self.policy.reset()
        ctx = EpisodeContext(task_name="order_series", episode_id=0)

        try:
            start_time = time.time()
            last_info = {}
            last_rew = torch.tensor(0.0, device=self.env.device)
            while True:
                with torch.no_grad():
                    obs_dict = self._obs_builder.build(
                        ctx=ctx,
                        step_id=step_i,
                        require=self._obs_require,
                        camera_names=self._camera_names,
                    )
                    actions = _get_action_from_policy(self.policy, obs_dict)
                    
                    # è½¬æ¢åŠ¨ä½œ (å†…éƒ¨ä¼šä¸´æ—¶å¼€å¯æ¢¯åº¦)
                    actions = self._convert_actions_by_control_mode(actions, obs_dict)
                    
                    obs, rew, terminated, truncated, info = self.env.step(actions)
                    last_info = info
                    last_rew = rew

                # ä»obs_dictå½•åˆ¶è§†é¢‘å¸§
                self._record_frame_from_obs(obs_dict)

                step_i += 1
                episode_length += 1

                if step_i % 100 == 0:
                    print(f"  step {step_i}: policyâ†’actionâ†’env.step ok, reward={last_rew.item():.4f}")

                # æ£€æŸ¥æ˜¯å¦ç»ˆæ­¢
                if terminated or truncated:
                    print(f"\nğŸ¯ Episode ç»ˆæ­¢: terminated={terminated}, truncated={truncated}")
                    break
                    
                if step_i % 1000 == 0:
                    isaac_env = self.env.unwrapped
                    robot_asset = isaac_env.scene.articulations["robot"]
                    default_state_tensor = robot_asset.data.root_state_w
                    print("\n" + "=" * 50)
                    print("Default Root State of 'robot' Asset:")
                    print(f"Shape: {default_state_tensor.shape}")
                    print(f"Data:\n{default_state_tensor[:, 0:3]}")
                    print(f"Reward :\n{rew}")
                    print("=" * 50 + "\n")
            
            # ä¿å­˜ç»“æœ
            self._save_evaluation_result(start_time, episode_length, last_info, last_rew, terminated, truncated, ctx)
            
        except KeyboardInterrupt:
            print("\nâ¹ï¸  è¯„ä¼°è¢«ç”¨æˆ·ä¸­æ–­")
            # ä¿å­˜ä¸­æ–­æ—¶çš„ç»“æœ
            self._save_evaluation_result(start_time, episode_length, last_info, last_rew, False, True, ctx)
            # ç«‹å³å…³é—­è§†é¢‘å½•åˆ¶
            print("ğŸ¬ ç«‹å³å…³é—­è§†é¢‘å½•åˆ¶")
            self.close_video_recording()
        except Exception as e:
            print(f"\nâŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            # ä¿å­˜é”™è¯¯æ—¶çš„ç»“æœ
            self._save_evaluation_result(start_time, episode_length, last_info, last_rew, False, True, ctx)
            # ç«‹å³å…³é—­è§†é¢‘å½•åˆ¶
            print("ğŸ¬ ç«‹å³å…³é—­è§†é¢‘å½•åˆ¶")
            self.close_video_recording()
        finally:
            # ç¡®ä¿è§†é¢‘å½•åˆ¶è¢«å…³é—­
            print("ğŸ¬ ç¡®ä¿è§†é¢‘å½•åˆ¶è¢«å…³é—­")
            self.close_video_recording()